{
  "cragEvaluationPrompt": "# Task: \\r\\nYou are given a Question, a model Prediction, and a list of Ground Truth answers, judge whether the model Prediction matches any answer from the list of Ground Truth answers. Follow the instructions step by step to make a judgement. \\r\\n1. If the model prediction matches any provided answers from the Ground Truth Answer list, \\\"Accuracy\\\" should be \\\"True\\\"; otherwise, \\\"Accuracy\\\" should be \\\"False\\\".\\r\\n2. If the model prediction says that it couldn't answer the question or it doesn't have enough information, \\\"Accuracy\\\" should always be \\\"False\\\".\\r\\n3. If the Ground Truth is \\\"invalid question\\\", \\\"Accuracy\\\" is \\\"True\\\" only if the model prediction is exactly \\\"invalid question\\\".\\r\\n# Output: \\r\\nRespond with only a single JSON string with an \\\"Accuracy\\\" field which is \\\"True\\\" or \\\"False\\\".\\r\\n# Examples:\\r\\nQuestion: how many seconds is 3 minutes 15 seconds?\\r\\nGround truth: [\\\"195 seconds\\\"]\\r\\nPrediction: 3 minutes 15 seconds is 195 seconds.\\r\\nAccuracy: True\\r\\n\\r\\nQuestion: Who authored The Taming of the Shrew (published in 2002)?\\r\\nGround truth: [\\\"William Shakespeare\\\", \\\"Roma Gill\\\"]\\r\\nPrediction: The author to The Taming of the Shrew is Roma Shakespeare.\\r\\nAccuracy: False\\r\\n\\r\\nQuestion: Who played Sheldon in Big Bang Theory?\\r\\nGround truth: [\\\"Jim Parsons\\\", \\\"Iain Armitage\\\"]\\r\\nPrediction: I am sorry I don't know.\\r\\nAccuracy: False"
}